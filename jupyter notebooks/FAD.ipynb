{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed997df0-dee5-4238-b89e-741da092f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from frechet_audio_distance import FrechetAudioDistance\n",
    "from scipy.io import wavfile\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.NSynthDataset import NSynthDataset\n",
    "from src.WaveGAN import WaveGANGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66de4b29-b4b3-495a-89c8-4114519854bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_size = 1000\n",
    "sr = 8191\n",
    "duration = 2\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf6e1091-a6da-471a-8669-56619b112df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../..')\n",
    "path = 'mnt/data/public/NSynth/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c51c3d69-8631-4a5a-895a-4c4980c18d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = NSynthDataset(data_path=path,  mel=False, pitched_z=True, \n",
    "                          sampling_rate=sr, duration=duration,\n",
    "                          min_class_count=10000, max_class_count=3000,\n",
    "                          z_size=z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e1f0935-e63e-4847-bfb0-e242cb70c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = NSynthDataset(data_path=path, mel=False,\n",
    "                          stage='test', pitched_z=True, \n",
    "                          sampling_rate=sr, duration=duration, \n",
    "                          cond_classes=train_set.cond_classes,\n",
    "                          z_size=z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb050c88-57a2-4e85-a42d-372f762f516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_set, batch_size=test_set.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fd0c78f-1397-401b-b0e7-2b99d45054fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WaveGANGenerator(\n",
       "  (fc): Linear(in_features=1006, out_features=16384, bias=True)\n",
       "  (deconv): Sequential(\n",
       "    (0): ConvTranspose1d(1024, 512, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose1d(512, 256, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose1d(256, 128, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose1d(128, 64, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "    (7): ReLU()\n",
       "    (8): ConvTranspose1d(64, 1, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "    (9): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = WaveGANGenerator(z_size, train_set.label_size, train_set.y_size, sr=sr, duration=duration).to(device)\n",
    "gen.load_state_dict(torch.load(f'users/adcy353/GANs-Conditional-Audio-Synthesis/models/wavegan/G_0.0001-1-826.pt'))\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5835e5-8640-48d4-af73-a46fd0873e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb543006-8192-406f-b04e-e86e227fc4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_path = 'users/adcy353/GANs-Conditional-Audio-Synthesis/frechet/pitched/'\n",
    "for i, (w, l, z) in enumerate(test_set):\n",
    "    s = gen.forward(z.unsqueeze(0).to(device), l.unsqueeze(0).to(device))\n",
    "    s.to(torch.device('cpu'))\n",
    "    s = s.detach().cpu()\n",
    "    \n",
    "    \n",
    "    output_wav_file = f\"{ev_path}output_{i}.wav\"\n",
    "    # Normalize the audio data to the appropriate range (-32768 to 32767 for 16-bit PCM)\n",
    "    normalized_audio = np.int16(s / max(np.abs(s)) * 32767)\n",
    "    # Write the WAV file\n",
    "    wavfile.write(output_wav_file, sr, normalized_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "467b729f-0c73-4832-b0b6-0eb6823fde78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /users/adcy353/.cache/torch/hub/harritaylor_torchvggish_master\n"
     ]
    }
   ],
   "source": [
    "# to use `vggish`\n",
    "frechet = FrechetAudioDistance(\n",
    "    model_name=\"vggish\",\n",
    "    use_pca=False, \n",
    "    use_activation=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c652cb79-2640-407c-803c-738cdbf81a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths to your saved embeddings\n",
    "background_embds_path = \"users/adcy353/GANs-Conditional-Audio-Synthesis/frechet/background/embeddings.npy\"\n",
    "eval_embds_path = \"users/adcy353/GANs-Conditional-Audio-Synthesis/frechet/pitched/embeddings.npy\"\n",
    "test_path = 'mnt/data/public/NSynth/nsynth-test/audio'\n",
    "\n",
    "# Compute FAD score while reusing the saved embeddings (or saving new ones if paths are provided and embeddings don't exist yet)\n",
    "fad_score = frechet.score(\n",
    "    test_path,\n",
    "    ev_path,\n",
    "    background_embds_path=background_embds_path,\n",
    "    eval_embds_path=eval_embds_path,\n",
    "    dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18f76993-0272-4e3a-827e-e5984a333caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.32707437765022"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fad_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb07697-c87b-4286-b986-7a0f6f310bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
